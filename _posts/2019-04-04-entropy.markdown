---
layout: post
title:  "熵(信息论)"
date:   2019-04-04 22:00:00 +0800
categories: statistical-concepts
---

## 熵
信息论中的熵的概念是由香农在他1948年的《数字通信理论》一文中提出的。熵是由随机数据源产生的信息速率。  
信息论的基本想法是越是对一件事情越了解，就越难获得新的信息。所以，如果一个事件越有可能发生，那么就会越感到它如果发生的话就不足为奇了。相反地，如果一个事件越未知，那么它将产生越多的新信息。所以，我们可以把熵看成获得新信息的平均信息量，也体现了变量状态未知量的一个测量，对变量状态多难预测的一个量化，变量随机性大小的刻画。比如，投掷一枚均匀骰子的熵比抛一枚均匀硬币的熵更大。  

### 信息量的定义 
我们这么定义离散型随机变量$X$的信息量：$I(X): I_X(\omega) = -log(P_X(\omega))$。它满足： 
（1）非负性$I_X(\omega) \geq 0$，且当事件为确定事件时，那么事件发生或预测中传达的信息量为0 ，因为我们已经知道它一定会发生了，所以不会获得更多的信息量了，即$I_X(\omega) = 0 \forall P(\omega) = 1$。  
（2）关于事件概率的逆序性（单调递减）：发生概率越大的事件的发生能提供的信息量越少；反之，稀少事件的发生能提供很大的信息（越惊奇）。  
（3）独立事件的信息量可加性：$I_{(X,Y)}(x,y)=I_X(x)I_Y(y)$  

### 熵的定义和性质
熵可定义为事件的平均信息量：$H(X)=E[I(X)] = - \sum_i P_i log(P_i)$。  
性质: 随机事件中等概率情况熵最大。

### 条件熵
条件熵是给定另一随机变量$X$的条件下，随机变量$Y$蕴含的平均信息量，记为$H(Y|X)$。其定义为：  
$$H(Y|X) = - \sum_{x,y}p(x,y)\log \frac{p(x,y)}{p(x)}$$
由以下推导得到：  
$H(Y|X) := \sum_x p(x)H(Y|X = x) = \sum_x p(x) \sum_y -p(y|x)\log p(y|x) = \sum_{(x,y)} -p(x,y)\log \frac{p(x,y)}{p(x)}$。  
可见，$H(Y|X) = H(X, Y) - H(X)$

### 互信息
在概率论和信息论中，互信息是对两个随机变量相互依赖的度量，它量化了在观察另一个随机变量下获得的关于该随机变量的信息量。它的定义为：$I(X, Y) = \sum_{(x,y)}p(x,y)\log \frac{p(x,y)}{p(x)p(y)}$。  
直观地，互信息衡量的是另一个随机变量给该随机变量带来的信息量的损失，也就是随机性的减少量。这与条件熵是相反的，条件熵衡量的是在知道另一个随机变量的情况下，该随机变量还存在的信息量，也就是保留的随机性大小。从中可看出，互信息其实就是随机变量之间共享的信息量。如果$X$和$Y$是独立的，那么知道$X$并不能提供$Y$的任何信息。  
性质：
非负性：$I(X, Y) \geq 0$ 由 $I(X, Y) = E[-\log \frac{p(x)p(y)}{p(x,y)}] \geq -\log E[\frac{p(x)p(y)}{p(x,y)}] = 0$得到。  
对称性：$I(X, Y) = I(Y, X)$ 可由定义得到。  


### 交叉熵（交互熵）
在信息论中，交叉熵表示的是编码方案不按真实的分布$p$走，而是按另一个优化分布$q$走。它定义为：
$H(p,q) = - \sum_x p(x) \log(q(x))$。  
来源：在信息论中，Kraft–McMillan理论提供了一个可直接解码的编码方案（$q(x_i) = {\frac{1}{2}}^{l_i}$ ），其预期的平均信息长度就是交叉熵。  
交叉熵往往会在机器学习深度学习中作为损失函数。通过考虑逻辑回归，我们有：  
真实概率可以表达为:$p_{y=1} = y$,$p_{y=0} = 1-y$，预测的概率为$q_{y=1} = \hat{y} = sigmoid(wx) = 1/(1+e^{-w \cdot x})$,$q_{y=0} = 1 - \hat{y}$，可通过交叉熵来衡量它们的相异性，且通过该特殊公式，得到‘$q$等于该特殊的$p$’当且仅当‘交叉熵等于0’。$H(p,q)=-y\log \hat{y} -(1-y)\log (1-\hat{y})$。

### 相对熵
相对熵也叫KL散度（Kullback-Leibler散度），用来度量一个概率分布与一个参考概率分布的有向差异。它定义为：
$D_{KL}(P\|Q) = - \sum_x P(x)\log \frac{Q(x)}{P(x)}$。  
通过该公式，它也等价$D_{KL}(P\|Q) = H(P, Q) - H(P)$。我们也可以看出，相对熵等于交叉熵与信息熵的差值。可以看出相对熵也是相对于最优编码方案的预期平均信息长度，备选编码方案的预期平均信息长度的额外付出的大小。  
由Jessen不等式，相对熵是非负的，且相对熵为0的情况为$P$和$Q$是相同分布。

#### 信息增益
在机器学习中，DL散度可被应用为信息增益。
